# -*- coding: utf-8 -*-
"""NA_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZZnL579j0eyvYLE88fEqrHnTaCoM5z1

Bias identification using Neural Attention Model:

  Components
    - Data processesing 
    - Word embedding
    - Model Encoding
    - Model Decoding
"""

# Install RAPIDS
!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!bash rapidsai-csp-utils/colab/rapids-colab.sh stable

import sys, os, shutil

sys.path.append('/usr/local/lib/python3.7/site-packages/')
os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'
os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'
os.environ["CONDA_PREFIX"] = "/usr/local"
for so in ['cudf', 'rmm', 'nccl', 'cuml', 'cugraph', 'xgboost', 'cuspatial']:
  fn = 'lib'+so+'.so'
  source_fn = '/usr/local/lib/'+fn
  dest_fn = '/usr/lib/'+fn
  if os.path.exists(source_fn):
    print(f'Copying {source_fn} to {dest_fn}')
    shutil.copyfile(source_fn, dest_fn)
# fix for BlazingSQL import issue
# ImportError: /usr/lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.26' not found (required by /usr/local/lib/python3.7/site-packages/../../libblazingsql-engine.so)
if not os.path.exists('/usr/lib64'):
    os.makedirs('/usr/lib64')
for so_file in os.listdir('/usr/local/lib'):
  if 'libstdc' in so_file:
    shutil.copyfile('/usr/local/lib/'+so_file, '/usr/lib64/'+so_file)
    shutil.copyfile('/usr/local/lib/'+so_file, '/usr/lib/x86_64-linux-gnu/'+so_file)

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_hub as hub
from keras import optimizers
import keras.backend as K
from keras.layers import Dropout, Dense
from keras.regularizers import l1, l2
from keras.models import Sequential

import logging

# get TF logger
log = logging.getLogger('tensorflow')
log.setLevel(logging.DEBUG)

# create formatter and add it to the handlers
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

# create file handler which logs even debug messages
fh = logging.FileHandler('tensorflow.log')
fh.setLevel(logging.DEBUG)
fh.setFormatter(formatter)
log.addHandler(fh)

#ABAE Model

embedding_dim = 512
units = 1024
batch_size = 64

#attention 
class Attention(tf.keras.Model):
  def __init__(self, embedding_dim, units, batch_size, maxlen):
    super(Attention, self).__init__()
    self.batch_size = batch_size
    self.units = units
    self.maxlen = maxlen
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(self.units,
                                   return_sequences=True,
                                   return_state=True,
                                   recurrent_initializer='glorot_uniform')
    self.softmax = tf.nn.softmax()  

  def feedforward(self, x, hidden):
    #x = self.embedding(x)
    x = tf.mean(self.embedding, (1,)).expand_dims(2)
    input1 = tf.linalg.matmul(self.gru, x)
    input2 = tf.linalg.matmul(self.embedding, input1).squeeze(2)
    #output, state = self.gru(x, initial_state=hidden)
    #return output, state
    output = self.softmax(input2)
    return output

  def initialize_hidden_state(self):
    return tf.zeros((self.batch_size, self.units))

#encoder = Encoder(df, embedding_dim, units, batch_size)

#input

#hidden_input = encoder.initialize_hidden_state()
#input = 
#context = tf.keras.layers.AdditiveAttention()([hidden_input, input])
#output = context

#class Bahdanau(tf.keras.layers.Layer):
    #def __init__(self, n):
        #super(Bahdanau, self).__init__()
        #self.w = tf.keras.layers.Dense(n)
        #self.u = tf.keras.layers.Dense(n)
        #self.v = tf.keras.layers.Dense(1)
    #def call(self, input1, h):
        #input1 = tf.expand_dims(input1, 1)
        #e = self.v(tf.nn.tanh(self.w(input1) + self.u(h)))
        #a = tf.nn.softmax(e, axis=1)
        #c = a * h
        #c = tf.reduce_sum(c, axis=1)
        #return a,c

## Say we want 10 units in the single layer MLP determining w,u
#attentionlayer = Bahdanau(10)
## Call with i/p: decoderstate @ t-1 and all encoder hidden states
#a, c = attentionlayer(input1, h)

class ABAE(tf.Module):
  def __init__ (self, embedding_dim, aspects, maxlen, regularizer):
    super(ABAE, self).__init__()
    self.embedding = embedding_dim
    self.aspects = aspects

    self.reg = regularizer
    self.maxlen = maxlen
    self.attention = Attention(self.embedding, maxlen)
    self.linear_layer = tfl.linear_layer(self.embedding, self.aspects)

    self.softmax = tf.nn.softmax() 
    self.aspect_embed = tf.Tensor(shape = (self.embedding, self.aspects))


  def get_aspects(self, text):
    w = self.attention(text)
    w_text_embed = tf.matmul(w.expand_dims(1),  
                                         text).squeeze()
    raw_importance = self.linear_layer(w_text_embed)
    aspects_importance = self.softmax(raw_importance)
    return w, w_text_embed, aspects_importance

  #def forward(self, text, negative_sample:

        # negative samples are averaged
    #mean_negative_samples = tf.math.reduce_mean(negative_sample, dim=2)

        # encoding: words embeddings -> sentence embedding, aspects importances
    #_, aspects_importance, w_text_embed = self.get_aspects(text)

        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding
    #recovered_emb = tf.matmul(self.aspects_embed, aspects_importance.expand_dims(2)).squeeze()

        # loss
    #reconstruction_triplet_loss = ABAE._reconstruction_loss(w_text_embed,
                                                            #recovered_emb,
                                                            #mean_negative_samples)
    #max_margin = tf.math.reduce_max(reconstruction_triplet_loss, tf.zeros_like(reconstruction_triplet_loss))

    #return self.reg * self._regularizer() + max_margin

   
  def _reconstruction_loss(w_text_embed, recovered_emb, mean_negative_emb):

    positive_dot_products = tf.matmul(w_text_embed.expand_dims(1), recovered_emb.expand_dims(2)).squeeze()
    negative_dot_products = tf.matmul(mean_negative_emb, recovered_emb.expand_dims(2)).squeeze()
    reconstruction_triplet_loss = tf.math.reduce_sum(1 - positive_dot_products.expand_dims(1) + negative_dot_products, dim=1)

    return reconstruction_triplet_loss

  def _regularizer(self):
      
    return tf.norm(
            tf.matmul(self.aspect_embed.t(), self.aspect_embed) \
            - tf.eye(self.aspects))
    
  def max_margin(self):

    reconstruction_triplet_loss = ABAE._reconstruction_loss(w_text_embed,
                                                            recovered_emb,
                                                            mean_negative_samples)
    max_margin = tf.math.reduce_max(reconstruction_triplet_loss, tf.zeros_like(reconstruction_triplet_loss))
    return self.reg * self._regularizer() + max_margin

  def compute_mask(self, input_tensor, mask=None):
      return None

  def get_output_shape_for(self, input_shape):
      return (input_shape[0][0], 1)

  def compute_output_shape(self, input_shape):
      return input_shape[0][0], 1

  def get_aspect_words(self, w2v_model, topn=15):
    words = []

        # getting aspects embeddings
    aspects = self.aspect_embed.detach().numpy()

        # getting scalar products of word embeddings and aspect embeddings;
        # to obtain the ``probabilities'', one should also apply softmax
    words_scores = w2v_model.wv.syn0.dot(aspects)

    for row in range(aspects.shape[1]):
        argmax_scalar_products = np.argsort(- words_scores[:, row])[:topn]
            # print([w2v_model.wv.index2word[i] for i in argmax_scalar_products])
            # print([w for w, dist in w2v_model.similar_by_vector(aspects.T[row])[:topn]])
        words.append([w2v_model.wv.index2word[i] for i in argmax_scalar_products])

    return words



#find ice cream lyrics as test of model performance

